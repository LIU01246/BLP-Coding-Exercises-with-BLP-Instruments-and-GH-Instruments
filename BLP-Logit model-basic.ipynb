{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECON 8601 - IO\n",
    "## Homework 3 - BLP Exercises\n",
    "### Xiang Liu\n",
    "#### Department of Economics, University of Minnesota"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains Berry-Levinsohn-Pakes (BLP) exefcises conducted in BLP (1995) including the logit model (a naive model of demand without the interaction between individual and product characteristics) and the model with interaction without and with BLP instruments. \n",
    "\n",
    "Additionally, where I use BLP instruments I add Gandhi-Houde (GH) instruments that introduced in Gandhi \\& Houde (2020) and compare the results.\n",
    "\n",
    "Please note that, this notebook only has codes that is intuitive to understand, but not performs very fast in terms of computational perspective. I'm working on a JIT (Just-In-Time) compilation in Python and a Julia version, which I expect both will perform much better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install numpy scipy pandas statsmodels scikit-learn matplotlib seaborn jax autograd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for numerical operations and statistics\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "\n",
    "# Advanced data structures\n",
    "import collections\n",
    "\n",
    "# Threading and multiprocessing\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Statistical models, GLM, and formula API\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Optimization\n",
    "from scipy import optimize\n",
    "\n",
    "# Automatic differentiation with autograd (choose one based on preference)\n",
    "# For basic use:\n",
    "import autograd.numpy as anp\n",
    "from autograd import grad\n",
    "\n",
    "# For more advanced use, including GPU support:\n",
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "\n",
    "import numba as nb\n",
    "from numba import jit\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  vehicle_name  year  horsepower_weight  length_width  ac_standard  \\\n",
      "0       AD100L    71           0.487908        1.2627            0   \n",
      "1       ADSUPE    71           0.465766        1.1136            0   \n",
      "2       AMAMBS    71           0.452489        1.6458            0   \n",
      "3       AMGREM    71           0.528997        1.1502            0   \n",
      "4       AMHORN    71           0.494324        1.2780            0   \n",
      "\n",
      "   miles_per_dollar     price  market_share  firmid  \n",
      "0          1.982720  8.876543      0.000281       7  \n",
      "1          2.201909  7.641975      0.000038       7  \n",
      "2          1.504286  8.928395      0.000442      15  \n",
      "3          1.888146  4.935803      0.001051      15  \n",
      "4          1.935989  5.516049      0.000670      15  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(\"cardata.txt\", delimiter='\\t')\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 - the Logit Model\n",
    "\n",
    "Assume utility is given by \n",
    "$$u_{ij} = \\alpha (y_i - p_j) + x_j\\beta + \\epsilon_{ij}, \\forall j = 0, 1, \\cdots, J_t$$\n",
    "with $\\epsilon_{ij}$ type 1 extreme value, $y_i$ income, $x_j, p_j$ observed characteristics, and parameters $\\beta, \\alpha$.\n",
    "\n",
    "Derive the appropriate likelihood function for the aggregated data by starting with the micro-level specification and regrouping. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The utility $\\delta_j$ for a product $j$ based on its characteristics $X$ and price $P$ is given by:\n",
    "\n",
    "$$\\delta_j(X, P, \\beta, \\alpha) = \\beta \\cdot X - \\alpha P$$\n",
    "\n",
    "The market share $s_j$ for a product $j$, derived from the utility, is calculated as:\n",
    "\n",
    "$$s_j = \\frac{e^{\\delta_j}}{1 + \\sum_{k} e^{\\delta_k}}$$\n",
    "\n",
    "Given observed market shares $\\hat{s}_j$, the objective function for likelihood maximization is:\n",
    "\n",
    "$$\\mathcal{L}(\\beta, \\alpha) = \\sum_{j} \\log(s_j) \\cdot \\hat{s}_j$$\n",
    "\n",
    "where $\\mathcal{L}(\\beta, \\alpha)$ is to be maximized with respect to $\\beta$ and $\\alpha$.\n",
    "\n",
    "Now, in order to do the following exercises, we convert these calculations into functions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def δ_j(X, P, β, α):\n",
    "    \"\"\"\n",
    "    Calculate the value based on characteristics, price, and their respective coefficients.\n",
    "    \n",
    "    Parameters:\n",
    "    X (np.array): Vector of characteristics.\n",
    "    P (float): Price.\n",
    "    β (np.array): Coefficients for characteristics.\n",
    "    α (float): Coefficient for price.\n",
    "    \n",
    "    Returns:\n",
    "    float: Calculated value.\n",
    "    \"\"\"\n",
    "    return np.dot(β, X) - α * P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def s_vec(df):\n",
    "    \"\"\"\n",
    "    Calculate the market share for all products based on δ values.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing product data and δ values.\n",
    "    \n",
    "    Returns:\n",
    "    np.array: Array of market shares for all products.\n",
    "    \"\"\"\n",
    "    markets = df['year'].unique()\n",
    "    results = OrderedDict()\n",
    "    \n",
    "    for j in markets:\n",
    "        # Filter df for the current market\n",
    "        market_df = df[df['year'] == j]\n",
    "        δ = market_df['δ_vec'].values\n",
    "        market_share = np.exp(δ) / (1 + np.sum(np.exp(δ)))\n",
    "        results[j] = market_share\n",
    "    \n",
    "    # Concatenate the market shares for all markets into a single array\n",
    "    all_market_shares = np.concatenate(list(results.values()))\n",
    "    return all_market_shares\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_func(s_j, s_hat_j):\n",
    "    \"\"\"\n",
    "    Calculate the product of the logarithm of estimated market share and observed market share.\n",
    "    \n",
    "    Parameters:\n",
    "    s_j (float): Estimated market share from s_vec.\n",
    "    s_hat_j (float): Observed market share for product j.\n",
    "    \n",
    "    Returns:\n",
    "    float: Result of the calculation, objective function.\n",
    "    \"\"\"\n",
    "    return np.log(s_j) * s_hat_j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_δ_vec(data, β, α, characteristics, constant=False):\n",
    "    \"\"\"\n",
    "    Compute utility for each vehicle.\n",
    "    \n",
    "    Parameters:\n",
    "    data (pd.DataFrame): DataFrame containing vehicle data.\n",
    "    β (np.array): Coefficients for characteristics.\n",
    "    α (float): Coefficient for price.\n",
    "    characteristics (list of str): Column names in 'data' corresponding to characteristics.\n",
    "    constant (bool): If True, adds a constant term to the characteristics vector.\n",
    "    \n",
    "    Returns:\n",
    "    list: A list of utility values (δ_vec) for each vehicle.\n",
    "    \"\"\"\n",
    "    δ_vec = []\n",
    "    for i in range(len(data)):\n",
    "        # Extract characteristics and price for each vehicle\n",
    "        X = [data.iloc[i][c] for c in characteristics]\n",
    "        if constant:\n",
    "            X = [1.0] + X\n",
    "        P = data.iloc[i]['price']\n",
    "\n",
    "        # Calculate utility and append to δ_vec\n",
    "        δ_vec.append(δ_j(X, P, β, α))\n",
    "    return δ_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_log_likelihood(β, α, data, characteristics):\n",
    "    \"\"\"\n",
    "    Compute the total log likelihood for the given parameters and data.\n",
    "    \n",
    "    Parameters:\n",
    "    β (np.array): Coefficients for characteristics.\n",
    "    α (float): Coefficient for price.\n",
    "    data (pd.DataFrame): DataFrame containing the data, must include 'market_share'.\n",
    "    characteristics (list of str): Column names in 'data' corresponding to characteristics.\n",
    "    \n",
    "    Returns:\n",
    "    float: The total log likelihood.\n",
    "    \"\"\"\n",
    "    total_ll = 0.0\n",
    "    \n",
    "    δ_vec = compute_δ_vec(data, β, α, characteristics)\n",
    "    data['δ_vec'] = δ_vec  # Add δ_vec as a new column to the DataFrame\n",
    "    \n",
    "    s_vec_estimated = s_vec(data)  # \n",
    "    \n",
    "    # Sum over j, products\n",
    "    for i in range(len(data)):\n",
    "        total_ll += obj_func(s_vec_estimated[i], data.iloc[i]['market_share'])\n",
    "        \n",
    "    return total_ll\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - MLE to the Logit Model\n",
    "\n",
    "Using the automobile data, estimate the logit demand specification using maximum likelihood and assuming prices are exogenous. What is the implied own-price elaticity of the 1990 Honda Accord (HDACCO)? What is the implied cross-elasticity of the Honda Accord with respect to the 1990 Ford Escort (FDESCO)? Pick two addtional cars and report the same numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slow codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "# # Initial parameter guesses\n",
    "# initial_β = np.array([0.1, 0.5, 0.3, 0.2])\n",
    "# initial_α = 0.4\n",
    "# characteristics = ['horsepower_weight', 'length_width', 'ac_standard', 'miles_per_dollar']\n",
    "\n",
    "# # Combine initial β and α into a single array for optimization\n",
    "# initial_params = np.concatenate([initial_β, [initial_α]])\n",
    "\n",
    "# # Define a wrapper function for the total_log_likelihood to fit minimize function requirements\n",
    "# def optimization_wrapper(params):\n",
    "#     β = params[:-1]\n",
    "#     α = params[-1]\n",
    "#     # Assuming df is defined and contains the necessary data\n",
    "#     return -total_log_likelihood(β, α, df, characteristics)\n",
    "\n",
    "# # Optimization\n",
    "# result = minimize(optimization_wrapper, initial_params)\n",
    "\n",
    "# # Extract optimized parameters\n",
    "# optimized_parameters = result.x\n",
    "# optimized_β = optimized_parameters[:-1]\n",
    "# optimized_α = optimized_parameters[-1]\n",
    "\n",
    "# print(\"Optimized β: \", \", \".join([f\"{b:.4f}\" for b in optimized_β]))\n",
    "# print(\"Optimized α: {:.4f}\".format(optimized_α))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numdifftools as nd\n",
    "from numpy.linalg import inv\n",
    "\n",
    "# # Assuming total_log_likelihood is defined and optimized_parameters are obtained from the optimization process\n",
    "\n",
    "# def neg_total_log_likelihood(params):\n",
    "#     β = params[:-1]\n",
    "#     α = params[-1]\n",
    "#     # Assuming df and characteristics are defined and accessible here\n",
    "#     return -total_log_likelihood(β, α, df, characteristics)\n",
    "\n",
    "# # Calculate the Hessian matrix of the negative log likelihood at the optimized parameters\n",
    "# H = nd.Hessian(neg_total_log_likelihood)(optimized_parameters)\n",
    "\n",
    "# # Invert the Hessian to get the covariance matrix, then take the diagonal (variances) and square root (standard errors)\n",
    "# se = np.sqrt(np.diag(inv(H)))\n",
    "\n",
    "# # Format and print the standard errors for β and α\n",
    "# se_β = \", \".join([f\"{b:.4f}\" for b in se[:-1]])\n",
    "# se_α = f\"{se[-1]:.4f}\"\n",
    "\n",
    "# print(\"SE β: \", se_β)\n",
    "# print(\"SE α: \", se_α)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make them a slightly faster if we combine them together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized β:  -0.0437, 1.8839, 0.1797, 0.1834\n",
      "Optimized α: 0.1357\n",
      "SE β:  11.0999, 3.9349, 3.1170, 1.9030\n",
      "SE α: 0.3270\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initial parameter setup\n",
    "initial_β = np.array([0.1, 0.5, 0.3, 0.2])\n",
    "initial_α = 0.4\n",
    "characteristics = ['horsepower_weight', 'length_width', 'ac_standard', 'miles_per_dollar']\n",
    "initial_params = np.concatenate([initial_β, [initial_α]])\n",
    "\n",
    "# Optimization wrapper function\n",
    "def optimization_wrapper(params):\n",
    "    β = params[:-1]\n",
    "    α = params[-1]\n",
    "    return -total_log_likelihood(β, α, df, characteristics)\n",
    "\n",
    "# Perform optimization\n",
    "result = minimize(optimization_wrapper, initial_params)\n",
    "\n",
    "# Extract optimized parameters\n",
    "optimized_parameters = result.x\n",
    "optimized_β = optimized_parameters[:-1]\n",
    "optimized_α = optimized_parameters[-1]\n",
    "\n",
    "# Hessian and SE calculation\n",
    "H = nd.Hessian(optimization_wrapper)(optimized_parameters)\n",
    "se = np.sqrt(np.diag(inv(H)))\n",
    "\n",
    "# Display results\n",
    "print(\"Optimized β: \", \", \".join([f\"{b:.4f}\" for b in optimized_β]))\n",
    "print(\"Optimized α: {:.4f}\".format(optimized_α))\n",
    "print(\"SE β: \", \", \".join([f\"{b:.4f}\" for b in se[:-1]]))\n",
    "print(\"SE α: {:.4f}\".format(se[-1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elasticities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elasticity(df, car1, car2, year, α_hat):\n",
    "    \"\"\"\n",
    "    Calculate own and cross price elasticities for any two given vehicles.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing vehicle data.\n",
    "    car1 (str): Vehicle name for which to calculate own and cross elasticity.\n",
    "    car2 (str): Vehicle name for cross elasticity calculation.\n",
    "    year (int): The year to filter the vehicles by.\n",
    "    α_hat (float): Estimated coefficient for price.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: Own and cross price elasticities.\n",
    "    \"\"\"\n",
    "    # Filter the DataFrame for the specific vehicles and year, \n",
    "    # and extract market share and price, as exogenously given\n",
    "    \n",
    "    s_j = df[(df['vehicle_name'] == car1) & (df['year'] == year)]['s'].iloc[0]\n",
    "    s_k = df[(df['vehicle_name'] == car2) & (df['year'] == year)]['s'].iloc[0]\n",
    "    p_j = df[(df['vehicle_name'] == car1) & (df['year'] == year)]['price'].iloc[0]\n",
    "    p_k = df[(df['vehicle_name'] == car2) & (df['year'] == year)]['price'].iloc[0]\n",
    "    \n",
    "    # Calculate own and cross price elasticities\n",
    "    elas_own = p_j / s_j * (-α_hat) * s_j * (1 - s_j)\n",
    "    elas_cros = p_k / s_j * α_hat * s_k * s_j\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"{car1} Own elasticity: {elas_own:.9f}\")\n",
    "    print(f\"{car1} {car2} Cross elasticity: {elas_cros:.9f}\")\n",
    "    \n",
    "    return elas_own, elas_cros\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract optimized β and α from optimized_parameters\n",
    "β_hat = optimized_parameters[:-1]  \n",
    "α_hat = optimized_parameters[-1]\n",
    "\n",
    "# Compute δ_vec using the compute_δ_vec function with the optimized parameters\n",
    "δ_vec = compute_δ_vec(df, β_hat, α_hat, characteristics)\n",
    "\n",
    "# Assign δ_vec to a new column in the DataFrame\n",
    "df['δ_vec'] = δ_vec  \n",
    "\n",
    "# Compute estimated s_vec using the s_vec function\n",
    "s_vec_estimated = s_vec(df)\n",
    "\n",
    "# Assign s_vec_estimated to a new column in the DataFrame\n",
    "df['s'] = s_vec_estimated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Own-price elasticity \n",
    "$$ \\frac{\\partial s_j}{\\partial p_j} \\times \\frac{p_j}{s_j} = \\frac{p_j}{s_j} \\times [-\\alpha s_j (1-s_j)] $$\n",
    "\n",
    "Cross-price elasticity \n",
    "$$ \\frac{\\partial s_j}{\\partial p_k} \\times \\frac{p_k}{s_j}=\\frac{p_k}{s_j} \\times \\alpha s_j s_k $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDACCO Own elasticity: -1.248946891\n",
      "HDACCO FDESCO Cross elasticity: 0.011376004\n",
      "CRDYNA Own elasticity: -1.333202079\n",
      "CRDYNA CRLANC Cross elasticity: 0.011559278\n",
      "CHSUMM Own elasticity: -1.012474398\n",
      "CHSUMM CHTC Cross elasticity: 0.003007558\n"
     ]
    }
   ],
   "source": [
    "# Compute elasticity for HDACCO and FDESCO\n",
    "elas_own, elas_cros = elasticity(df, \"HDACCO\", \"FDESCO\", 90, α_hat)\n",
    "\n",
    "# Compute elasticity for CRDYNA and CRLANC\n",
    "elas_own, elas_cros = elasticity(df, \"CRDYNA\", \"CRLANC\", 89, α_hat)\n",
    "\n",
    "# Compute elasticity for CHSUMM and CHTC\n",
    "elas_own, elas_cros = elasticity(df, \"CHSUMM\", \"CHTC\", 89, α_hat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 - Linear Regression of the Logit Model\n",
    "\n",
    "Estimate the logit demand specification using the linearized version of this model from BLP (i.e. regress $\\log (s_j - s_0)$ on characteristics). What is the implied own-prince elaticity of the 1990 Honda Accord (HDACCO)? What is the implied cross-elasticity of the Honda Accord with respect to the 1990 Ford Escort (FDESCO)? Pick two addtional cars and report the same numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the DataFrame by 'year' and calculate s_0 for each group/year\n",
    "df['s_0'] = df.groupby('year')['market_share'].transform(lambda x: 1 - x.sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "β_hat:  -10.0716, -0.1242, 2.3421, -0.0343, 0.2650\n",
      "α_hat: 0.0886\n",
      "SE β:  0.2529, 0.2773, 0.1252, 0.0728, 0.0431\n",
      "SE α: 0.0040\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare X: Add a column of ones to df for the intercept, then select the required columns\n",
    "X = np.hstack([np.ones((df.shape[0], 1)), df[['horsepower_weight', 'length_width', 'ac_standard', 'miles_per_dollar', 'price']].values])\n",
    "\n",
    "# Create the vector Y\n",
    "Y = np.log(df['market_share'].values) - np.log(df['s_0'].values)\n",
    "\n",
    "# Manual OLS regression using the Normal Equation to compute β\n",
    "β_manual = np.linalg.inv(X.T @ X) @ X.T @ Y\n",
    "\n",
    "# Extract β_hat and α_hat from the manual calculation\n",
    "β_hat_manual = β_manual[:-1]\n",
    "α_hat_manual = -β_manual[-1]\n",
    "\n",
    "# Use statsmodels to fit the model and extract standard errors\n",
    "model = sm.OLS(Y, X)\n",
    "OLS_results = model.fit()\n",
    "\n",
    "# Standard errors\n",
    "se_β = OLS_results.bse[:-1]  # Standard errors for β\n",
    "se_α = OLS_results.bse[-1]  # Standard error for α\n",
    "\n",
    "# Printing results\n",
    "print(\"β_hat: \", \", \".join([f\"{b:.4f}\" for b in β_hat_manual]))\n",
    "print(\"α_hat: {:.4f}\".format(α_hat_manual))\n",
    "print(\"SE β: \", \", \".join([f\"{se:.4f}\" for se in se_β]))\n",
    "print(\"SE α: {:.4f}\".format(se_α))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statsmodels Summary:\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.387\n",
      "Model:                            OLS   Adj. R-squared:                  0.386\n",
      "Method:                 Least Squares   F-statistic:                     279.2\n",
      "Date:                Sat, 09 Mar 2024   Prob (F-statistic):          6.52e-232\n",
      "Time:                        20:30:09   Log-Likelihood:                -3319.3\n",
      "No. Observations:                2217   AIC:                             6651.\n",
      "Df Residuals:                    2211   BIC:                             6685.\n",
      "Df Model:                           5                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const        -10.0716      0.253    -39.823      0.000     -10.568      -9.576\n",
      "x1            -0.1242      0.277     -0.448      0.654      -0.668       0.420\n",
      "x2             2.3421      0.125     18.707      0.000       2.097       2.588\n",
      "x3            -0.0343      0.073     -0.472      0.637      -0.177       0.108\n",
      "x4             0.2650      0.043      6.145      0.000       0.180       0.350\n",
      "x5            -0.0886      0.004    -22.015      0.000      -0.097      -0.081\n",
      "==============================================================================\n",
      "Omnibus:                      157.932   Durbin-Watson:                   1.417\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              205.482\n",
      "Skew:                          -0.632   Prob(JB):                     2.40e-45\n",
      "Kurtosis:                       3.792   Cond. No.                         203.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Display the summary from statsmodels\n",
    "print(\"\\nStatsmodels Summary:\")\n",
    "print(OLS_results.summary())\n",
    "\n",
    "# Extract β_hat and α_hat from statsmodels results for comparison\n",
    "    # Do not skip the constant term for β \n",
    "    # since it's useful to calculate the elasticities\n",
    "β_hat = OLS_results.params[:-1]  \n",
    "\n",
    "\n",
    "α_hat = -OLS_results.params[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elasticities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDACCO Own elasticity: -0.823079102\n",
      "HDACCO FDESCO Cross elasticity: 0.000451967\n",
      "CRDYNA Own elasticity: -0.878261183\n",
      "CRDYNA CRLANC Cross elasticity: 0.000526359\n",
      "CHSUMM Own elasticity: -0.667692045\n",
      "CHSUMM CHTC Cross elasticity: 0.000228044\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Compute δ vector and update df\n",
    "δ_vec = compute_δ_vec(df, β_hat, α_hat, characteristics, constant=True)\n",
    "df['δ_vec'] = δ_vec  # Directly assign the computed δ vector to a new column\n",
    "\n",
    "# Step 2: Calculate estimated market shares and update df\n",
    "s_vec_estimated = s_vec(df)\n",
    "df['s'] = s_vec_estimated  # Directly assign the estimated market shares to a new column\n",
    "\n",
    "# Step 3: Calculate elasticities for specified product pairs\n",
    "elas_own_hdacco_fdesco, elas_cros_hdacco_fdesco = elasticity(df, \"HDACCO\", \"FDESCO\", 90, α_hat)\n",
    "elas_own_crdyna_crlanc, elas_cros_crdyna_crlanc = elasticity(df, \"CRDYNA\", \"CRLANC\", 89, α_hat)\n",
    "elas_own_chsumm_chtc, elas_cros_chsumm_chtc = elasticity(df, \"CHSUMM\", \"CHTC\", 89, α_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.1 - BLP Instruments in Logit Model\n",
    "\n",
    "Write\n",
    "\n",
    "$$u_{ij} = \\alpha \\ln(y_i – p_j) + x_j \\beta + \\xi_j + \\epsilon_{ij}, \\forall j = 0, 1, \\cdots, J.$$\n",
    "\n",
    "Use the instruments used in BLP. You will nned the firmids and the year variables to calculate  these instruments (they are product-firm-year specific). Estimate the logit model using 2SLS and instrumenting for price. \n",
    "\n",
    "What is the implied own-prince elaticity of the 1990 Honda Accord (HDACCO)? What is the implied cross-elasticity of the Honda Accord with respect to the 1990 Ford Escort (FDESCO)? Pick two addtional cars and report the same numbers. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define BLP Instruments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_instrument(data, characteristic):\n",
    "    # Define new column names based on the characteristic\n",
    "    own_firm_sum = f\"own_firm_sum_{characteristic}\"\n",
    "    rival_firm_sum = f\"rival_firm_sum_{characteristic}\"\n",
    "    \n",
    "    # Initialize the new columns with zeros\n",
    "    data[own_firm_sum] = 0.0\n",
    "    data[rival_firm_sum] = 0.0\n",
    "    \n",
    "    # Iterate over the DataFrame by index\n",
    "    for i in data.index:\n",
    "        current_firm = data.at[i, 'firmid']\n",
    "        current_time = data.at[i, 'year']\n",
    "        \n",
    "        # Filter data for the current firm and time period\n",
    "        own_firm_data = data[(data['firmid'] == current_firm) & (data['year'] == current_time)]\n",
    "        rival_firm_data = data[(data['firmid'] != current_firm) & (data['year'] == current_time)]\n",
    "        \n",
    "        # Sum of characteristic for own-firm products (excluding current product)\n",
    "        data.at[i, own_firm_sum] = own_firm_data[characteristic].sum() - data.at[i, characteristic]\n",
    "        \n",
    "        # Sum of characteristic for rival-firm products\n",
    "        data.at[i, rival_firm_sum] = rival_firm_data[characteristic].sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the characteristics\n",
    "characteristics = ['horsepower_weight', 'length_width', 'ac_standard', 'miles_per_dollar']\n",
    "\n",
    "# Loop through characteristics to construct instruments\n",
    "for char in characteristics:\n",
    "    construct_instrument(df, char)\n",
    "\n",
    "# Define the instrumental variables\n",
    "instruments = characteristics + \\\n",
    "              [f\"own_firm_sum_{char}\" for char in characteristics] + \\\n",
    "              [f\"rival_firm_sum_{char}\" for char in characteristics]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame columns to arrays/matrices\n",
    "Y = np.log(df['market_share'].values) - np.log(df['s_0'].values)\n",
    "constant = np.ones(df.shape[0])\n",
    "\n",
    "# Endogenous variable\n",
    "X_endogenous = df[['price']].values\n",
    "\n",
    "# Exogenous variables\n",
    "X_exogenous = df[characteristics].values\n",
    "X_exogenous = np.hstack([constant.reshape(-1, 1), X_exogenous])\n",
    "\n",
    "# Instrumental variables\n",
    "Z = df[instruments].values\n",
    "Z = np.hstack([constant.reshape(-1, 1), Z])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install linearmodels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from linearmodels.iv import IV2SLS\n",
    "\n",
    "# # Prepare the regressor and instrument matrices\n",
    "# X = np.hstack([X_exogenous, X_endogenous])  # Combine exogenous and endogenous variables\n",
    "\n",
    "# # Perform the IV 2SLS regression\n",
    "# iv_model = IV2SLS(dependent=Y, \n",
    "#                   exog=X_exogenous,  # Only exogenous variables\n",
    "#                   endog=X_endogenous,  # Endogenous variable(s)\n",
    "#                   instruments=Z).fit()\n",
    "\n",
    "# # Extract results\n",
    "# β_hat = iv_model.params[:-1]  # Exclude the last one for α_hat\n",
    "# α_hat = iv_model.params[-1]   # The last parameter is α_hat\n",
    "\n",
    "# # Extract standard errors\n",
    "# se_β = iv_model.std_errors[:-1]  # Standard errors for β_hat\n",
    "# se_α = iv_model.std_errors[-1]   # Standard error for α_hat\n",
    "\n",
    "# # Printing results\n",
    "# print(\"β_hat: \", \", \".join([f\"{b:.3f}\" for b in β_hat]))\n",
    "# print(\"α_hat: {:.3f}\".format(α_hat))\n",
    "# print(\"SE β: \", \", \".join([f\"{se:.3f}\" for se in se_β]))\n",
    "# print(\"SE α: {:.3f}\".format(se_α))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does not work since ValueError: instruments [exog instruments]  do not have full column rank. Some thing just happened and I need to deal with it later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "β_hat:  -9.902, 1.345, 2.287, 0.532, 0.163\n",
      "α_hat: -0.140\n",
      "SE β:  0.264, 0.407, 0.130, 0.134, 0.049\n",
      "SE α: 0.011\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from statsmodels.sandbox.regression.gmm import IV2SLS\n",
    "\n",
    "\n",
    "# Prepare the full regressor matrix including a constant, exogenous, and endogenous variables\n",
    "X = np.hstack([X_exogenous, X_endogenous])  # This includes the constant and exogenous variables\n",
    "\n",
    "# Perform the IV 2SLS regression\n",
    "# Note: statsmodels expects endog (Y), exog (X including constant and exogenous variables),\n",
    "# and instrument (Z including constant and instruments) as parameters\n",
    "iv_model = IV2SLS(endog=Y, exog=X, instrument=Z).fit()\n",
    "\n",
    "# Extract results and standard errors\n",
    "β_hat = iv_model.params[:-1]  # Excluding the last one for α_hat\n",
    "α_hat = iv_model.params[-1]   # The last parameter is assumed to be α_hat\n",
    "se = iv_model.bse  # Standard errors of the estimated parameters\n",
    "\n",
    "# Printing results\n",
    "print(\"β_hat: \", \", \".join([f\"{b:.3f}\" for b in β_hat]))\n",
    "print(\"α_hat: {:.3f}\".format(α_hat))\n",
    "print(\"SE β: \", \", \".join([f\"{se:.3f}\" for se in se[:-1]]))\n",
    "print(\"SE α: {:.3f}\".format(se[-1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elaticities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDACCO Own elasticity: 1.299306150\n",
      "HDACCO FDESCO Cross elasticity: -0.000093166\n",
      "CRDYNA Own elasticity: 1.386705671\n",
      "CRDYNA CRLANC Cross elasticity: -0.000041518\n",
      "CHSUMM Own elasticity: 1.054232826\n",
      "CHSUMM CHTC Cross elasticity: -0.002871674\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Compute δ vector and update DataFrame\n",
    "δ_vec = compute_δ_vec(df, β_hat, α_hat, characteristics, constant=True)\n",
    "df['δ_vec'] = δ_vec\n",
    "\n",
    "# Step 2: Calculate estimated market shares and update DataFrame\n",
    "s_vec_estimated = s_vec(df)\n",
    "df['s'] = s_vec_estimated\n",
    "\n",
    "# Step 3: Calculate elasticities for specified product pairs\n",
    "elas_own_hdacco_fdesco, elas_cros_hdacco_fdesco = elasticity(df, \"HDACCO\", \"FDESCO\", 90, α_hat)\n",
    "elas_own_crdyna_crlanc, elas_cros_crdyna_crlanc = elasticity(df, \"CRDYNA\", \"CRLANC\", 89, α_hat)\n",
    "elas_own_chsumm_chtc, elas_cros_chsumm_chtc = elasticity(df, \"CHSUMM\", \"CHTC\", 89, α_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4.2 - Gandhi-Houde (DH) Instrumnets in Logit Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to calculate GH IV, only construct quadratic instruments without interaction terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_IV_df(firm_distances, group, prefix):\n",
    "    for char in firm_distances.keys():\n",
    "        # Calculate row sums of the squared distances matrices\n",
    "        row_sums = np.sum(firm_distances[char], axis=1)\n",
    "        \n",
    "        # Creating new column names based on the prefix and characteristic name\n",
    "        column_name = prefix + char\n",
    "        \n",
    "        # Assign the row sums to the group DataFrame as new columns\n",
    "        group[column_name] = row_sums\n",
    "\n",
    "    return group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate GH IV, I only construct quadratic instruments without interaction terms\n",
    "\n",
    "def GH_IV(df, characteristics):\n",
    "    # Group the DataFrame by 'year'\n",
    "    grouped_data = df.groupby('year')\n",
    "    collected_groups = []\n",
    "\n",
    "    for year, group in grouped_data:\n",
    "        firm_intra_distances = {}\n",
    "        firm_inter_distances = {}\n",
    "\n",
    "        for char in characteristics:\n",
    "            n = len(group)\n",
    "            intra_distance_matrix = np.zeros((n, n))\n",
    "            inter_distance_matrix = np.zeros((n, n))\n",
    "\n",
    "            # Iterate through each pair of rows in the group to calculate distances\n",
    "            for i in range(n):\n",
    "                for j in range(n):\n",
    "                    # Intrafirm distance\n",
    "                    if group.iloc[i]['firmid'] == group.iloc[j]['firmid']:\n",
    "                        intra_distance_matrix[i, j] = group.iloc[i][char] - group.iloc[j][char]\n",
    "                    # Interfirm distance\n",
    "                    elif group.iloc[i]['firmid'] != group.iloc[j]['firmid']:\n",
    "                        inter_distance_matrix[i, j] = group.iloc[i][char] - group.iloc[j][char]\n",
    "\n",
    "            firm_intra_distances[char] = intra_distance_matrix ** 2\n",
    "            firm_inter_distances[char] = inter_distance_matrix ** 2\n",
    "\n",
    "        group = convert_IV_df(firm_intra_distances, group, \"intra_IV_\")\n",
    "        group = convert_IV_df(firm_inter_distances, group, \"inter_IV_\")\n",
    "        collected_groups.append(group)\n",
    "\n",
    "    final_df = pd.concat(collected_groups)\n",
    "\n",
    "    return final_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = GH_IV(df, characteristics)\n",
    "\n",
    "# Constructing GH_instruments list with original characteristics and the generated intra- and inter- IV names\n",
    "GH_instruments = characteristics + \\\n",
    "                 [\"intra_IV_\" + char for char in characteristics] + \\\n",
    "                 [\"inter_IV_\" + char for char in characteristics]\n",
    "\n",
    "# GH_instruments now contains all the required columns names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for IV2SLS\n",
    "\n",
    "X = np.hstack([X_exogenous, X_endogenous])  # This includes the constant and exogenous variables\n",
    "\n",
    "# Instrumental variables\n",
    "Z = df[GH_instruments].values\n",
    "Z = np.hstack([constant.reshape(-1, 1), Z])\n",
    "\n",
    "# Fit the model # Perform the IV 2SLS regression\n",
    "\n",
    "iv_model_GH = IV2SLS(endog=Y, exog=X, instrument=Z).fit()\n",
    "\n",
    "\n",
    "# Extract the estimated coefficients and standard errors\n",
    "β_hat = iv_model_GH.params[:-1]\n",
    "α_hat = -iv_model_GH.params[-1]\n",
    "se_beta = iv_model_GH.bse[:-1]\n",
    "se_alpha = iv_model_GH.bse[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "β_hat: -9.908, 1.292, 2.289, 0.512, 0.167\n",
      "α_hat: 0.138\n",
      "SE β: 0.263, 0.361, 0.130, 0.113, 0.047\n",
      "SE α: 0.009\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"β_hat: {', '.join([f'{x:.3f}' for x in β_hat])}\")\n",
    "print(f\"α_hat: {α_hat:.3f}\")\n",
    "print(f\"SE β: {', '.join([f'{x:.3f}' for x in se_beta])}\")\n",
    "print(f\"SE α: {se_alpha:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDACCO Own elasticity: -1.281617801\n",
      "HDACCO FDESCO Cross elasticity: 0.000699002\n",
      "CRDYNA Own elasticity: -1.367689576\n",
      "CRDYNA CRLANC Cross elasticity: 0.000703968\n",
      "CHSUMM Own elasticity: -1.039727024\n",
      "CHSUMM CHTC Cross elasticity: 0.000353852\n"
     ]
    }
   ],
   "source": [
    "# Assuming compute_δ_vec, s_vec, and elasticity functions are defined elsewhere\n",
    "δ_vec = compute_δ_vec(df, β_hat, α_hat, characteristics, constant=True)\n",
    "df['δ_vec'] = δ_vec\n",
    "\n",
    "s_vec_estimated = s_vec(df)\n",
    "df['s'] = s_vec_estimated\n",
    "\n",
    "# For elasticity calculations\n",
    "elas_own, elas_cros = elasticity(df, \"HDACCO\", \"FDESCO\", 90, α_hat)\n",
    "elas_own, elas_cros = elasticity(df, \"CRDYNA\", \"CRLANC\", 89, α_hat)\n",
    "elas_own, elas_cros = elasticity(df, \"CHSUMM\", \"CHTC\", 89, α_hat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5.1 - BLP Instruments in Logit Model with Random Coefficients\n",
    "\n",
    "Write\n",
    "\n",
    "$$u_{ij} = \\alpha \\ln(y_i – p_j) + x_j \\beta_i + \\xi_j + \\epsilon_{ij}, \\forall j = 0, 1, \\cdots, J.$$\n",
    "\n",
    "Now add random coefficients for each characteristic and estimate the means and variances of these nromally distributed random coefficients. Estimate the demand side of the model only (unlesss you are ambitious and want smaller standard errors - then add the supply side too).\n",
    "\n",
    "What is the implied own-prince elaticity of the 1990 Honda Accord (HDACCO)? What is the implied cross-elasticity of the Honda Accord with respect to the 1990 Ford Escort (FDESCO)? Pick two addtional cars and report the same numbers. \n",
    "\n",
    "This part is unfinished, see Julia version for random coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_random(characteristics=['horsepower_weight', 'length_width', 'ac_standard', 'miles_per_dollar']):\n",
    "    # Number of people\n",
    "    N = 100\n",
    "\n",
    "    # Create an empty DataFrame\n",
    "    people_df = pd.DataFrame()\n",
    "\n",
    "    # Set the random seed for reproducibility, equivalent to Julia's MersenneTwister\n",
    "    np.random.seed(0)\n",
    "\n",
    "    # Generate and add data for each characteristic\n",
    "    for char in characteristics:\n",
    "        people_df[char] = np.random.normal(0, 1, N)\n",
    "\n",
    "    return people_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for estimated share for one market\n",
    "\n",
    "def eshare(char_matrix, δ, rand_sample, σ):\n",
    "    \"\"\"\n",
    "    Estimate the market share for one market.\n",
    "\n",
    "    char_matrix: Matrix of characteristics for products (products x characteristics).\n",
    "    δ: Vector of δ_i (intrinsic utility) for each product in market t.\n",
    "    rand_sample: Matrix of random coefficients for each individual (individuals x characteristics).\n",
    "    σ: Vector of scaling parameters for the random coefficients.\n",
    "    \n",
    "    return: Vector of estimated shares for each product.\n",
    "    \"\"\"\n",
    "    R = rand_sample.shape[0]  # Number of simulated individuals\n",
    "    k = char_matrix.shape[0]  # Number of products\n",
    "    share = np.zeros(k)\n",
    "    \n",
    "    # s_jt = 1/R * sum_i{ exp(δ_jt + μ_ijt)/sum_j'{exp(δ_j't + μ_ij't)} }\n",
    "    for i in range(R):\n",
    "        # Calculate exp(δ_jt + μ_ijt) where μ_ijt = σ_1*v_i1*char_1 + ... + σ_k*v_ik*char_k\n",
    "        numerator = np.exp(δ + char_matrix @ (rand_sample[i, :] * σ))\n",
    "        denominator = 1 + np.sum(numerator)\n",
    "        prob = numerator / denominator\n",
    "        share += 1/R * prob\n",
    "\n",
    "    return share\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for estimating delta for one market\n",
    "\n",
    "def cdelta(char_matrix, rand_sample, share, σ, tol=1e-12):\n",
    "    \"\"\"\n",
    "    Estimate delta for one market.\n",
    "    \n",
    "    char_matrix: Matrix of characteristics for products (products x characteristics).\n",
    "    rand_sample: Matrix of random coefficients for each individual (individuals x characteristics).\n",
    "    share: Vector of actual market shares for each product.\n",
    "    σ: Vector of scaling parameters for the random coefficients.\n",
    "    tol: Tolerance level for convergence.\n",
    "    \n",
    "    return: Vector of estimated delta values for each product.\n",
    "    \"\"\"\n",
    "    k = char_matrix.shape[0]\n",
    "    δ_0 = np.ones(k)\n",
    "    δ_1 = np.ones(k)\n",
    "    error = 1\n",
    "\n",
    "    while error > tol:\n",
    "        δ_0 = δ_1\n",
    "        estimate_share = eshare(char_matrix, δ_0, rand_sample, σ)\n",
    "        δ_1 = δ_0 + np.log(share) - np.log(estimate_share)\n",
    "        error = np.dot((δ_1 - δ_0), (δ_1 - δ_0))\n",
    "\n",
    "    return δ_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_full(df, rand_sample, characteristics, σ, tol=1e-12):\n",
    "    markets = df['year'].unique()  # Get unique market identifiers\n",
    "    results = OrderedDict()\n",
    "    err = OrderedDict()\n",
    "\n",
    "    for j in markets:\n",
    "        # Filter DataFrame for the current market more explicitly\n",
    "        market_df = df.loc[df['year'] == j]\n",
    "\n",
    "        # Ensure characteristics is a list to prevent potential issues\n",
    "        if not isinstance(characteristics, list):\n",
    "            characteristics = list(characteristics)\n",
    "\n",
    "        # Extract characteristics and market share for the current market\n",
    "        chars = market_df.loc[:, characteristics].to_numpy()\n",
    "        share = market_df['market_share'].to_numpy()\n",
    "\n",
    "        # Estimate delta for the current market\n",
    "        δ_new = cdelta(chars, rand_sample, share, σ, tol=tol)\n",
    "        results[j] = δ_new\n",
    "\n",
    "        # Check for NaN values in the results\n",
    "        if np.isnan(δ_new).any():\n",
    "            err[j] = 'NaN values detected'\n",
    "\n",
    "    # Combine the results from each market into a single array\n",
    "    combined_results = np.concatenate(list(results.values()))\n",
    "\n",
    "    return combined_results, err\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj(df, rand_sample, characteristics, IV_list, σ, delta_full_func=delta_full):\n",
    "    # Calculate the delta (δ) value using the provided or default delta_full function\n",
    "    δ, _ = delta_full_func(df, rand_sample, characteristics, σ)\n",
    "    \n",
    "    # Extracting the instrumental variables (IV) matrix from the dataframe.\n",
    "    z1 = df.loc[:, IV_list].values\n",
    "    \n",
    "    # Ensuring the 'price' column is included in the characteristics if not already\n",
    "    chars = characteristics\n",
    "    if 'price' not in characteristics:\n",
    "        chars = characteristics + ['price']\n",
    "    \n",
    "    # Creating the design matrix x with a constant term and characteristics\n",
    "    x = np.hstack([np.ones((df.shape[0], 1)), df.loc[:, chars].values])\n",
    "    \n",
    "    # IV matrix with a constant term added\n",
    "    z = np.hstack([np.ones((df.shape[0], 1)), z1])\n",
    "    \n",
    "    # Weight matrix for the instrumental variables\n",
    "    w = np.linalg.inv(z.T @ z)\n",
    "    \n",
    "    # Estimating beta (β) using the 2SLS method\n",
    "    β = np.linalg.inv(x.T @ z @ w @ z.T @ x) @ (x.T @ z @ w @ z.T @ δ)\n",
    "    \n",
    "    # Calculating the residuals\n",
    "    u = δ - x @ β\n",
    "    \n",
    "    # Calculating the result as u'ZWZ'u\n",
    "    res = u.T @ z @ w @ z.T @ u\n",
    "    \n",
    "    return res, β, u\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define characteristics without the price since there's no random coefficient for price\n",
    "characteristics = [\"horsepower_weight\", \"length_width\", \"ac_standard\", \"miles_per_dollar\"]\n",
    "\n",
    "# Generate random sample using the characteristics\n",
    "rand_sample = gen_random(characteristics)  # Ensure this returns a NumPy array or similar\n",
    "\n",
    "# Construct instruments for each characteristic in the dataframe\n",
    "for char in characteristics:\n",
    "    construct_instrument(df, char)\n",
    "\n",
    "# Create the IV list by concatenating the characteristics with their own and rival firm sums\n",
    "IV_list = characteristics + \\\n",
    "          [\"own_firm_sum_\" + char for char in characteristics] + \\\n",
    "          [\"rival_firm_sum_\" + char for char in characteristics]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['horsepower_weight',\n",
       " 'length_width',\n",
       " 'ac_standard',\n",
       " 'miles_per_dollar',\n",
       " 'own_firm_sum_horsepower_weight',\n",
       " 'own_firm_sum_length_width',\n",
       " 'own_firm_sum_ac_standard',\n",
       " 'own_firm_sum_miles_per_dollar',\n",
       " 'rival_firm_sum_horsepower_weight',\n",
       " 'rival_firm_sum_length_width',\n",
       " 'rival_firm_sum_ac_standard',\n",
       " 'rival_firm_sum_miles_per_dollar']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IV_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
